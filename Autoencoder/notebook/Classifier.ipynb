{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXSMBDeSY0In"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU6Dn8jSaRvG"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1257hHZT8en",
        "outputId": "f7785ff8-e945-4c93-bf07-c97a4b2e10b8"
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import Sequential, optimizers\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "  raise SystemError(\"GPU device not found\")\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxtMSdUQY2AQ"
      },
      "source": [
        "###Setup input and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EEAnH_pUkBM"
      },
      "source": [
        "configuration = (64, 25, 16)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxM5ODU4aWaE"
      },
      "source": [
        "###Read Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAVE2HvRUxGQ"
      },
      "source": [
        "def parse_dataset(filepath):\n",
        "    \"\"\" function used to parse the data of a dataset \"\"\"\n",
        "\n",
        "    # open the dataset\n",
        "    with open(filepath, \"rb\") as dataset:\n",
        "        # read the magic number and the number of images\n",
        "        magic_number, number_of_images = struct.unpack(\">II\", dataset.read(8))\n",
        "        # read the number of rows and number of columns per image\n",
        "        rows, columns = struct.unpack(\">II\", dataset.read(8))\n",
        "        # now read the rest of the file using numpy.fromfile()\n",
        "        images = np.fromfile(dataset, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
        "        # reshape so that the final shape is (number_of_images, rows, columns)\n",
        "        images = images.reshape((number_of_images, rows, columns))\n",
        "\n",
        "    # return the images\n",
        "    return images\n",
        "\n",
        "\n",
        "def parse_labelset(filepath):\n",
        "    \"\"\" function used to parse the data of a labelset \"\"\"\n",
        "\n",
        "    # open the file\n",
        "    with open(filepath, \"rb\") as labelset:\n",
        "        # read the magic number and the number of labels\n",
        "        magic_number, number_of_labels = struct.unpack(\">II\", labelset.read(8))\n",
        "        # now read the rest of the file using numpy.fromfile()\n",
        "        labels = np.fromfile(labelset, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
        "\n",
        "    # return the labels\n",
        "    return labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny2XNlKlU-Pr",
        "outputId": "c9a78f78-87b9-4ed1-e226-0d4bb8be8993"
      },
      "source": [
        "# EDIT THE PATHS OF THE DATASETS HERE\n",
        "train_images_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Uni\", \"Project\", \"training.dat\")\n",
        "train_labels_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Uni\", \"Project\", \"training_labels.dat\")\n",
        "\n",
        "# LOAD THE DATASETS HERE\n",
        "X = parse_dataset(train_images_path)\n",
        "Y = parse_labelset(train_labels_path)\n",
        "lb = LabelBinarizer()\n",
        "Y = lb.fit_transform(Y)\n",
        "\n",
        "# GET USEFUL VARIABLES\n",
        "rows = X.shape[1]\n",
        "columns = X.shape[2]\n",
        "\n",
        "X = X.reshape(-1, rows, columns, 1)\n",
        "# normalize\n",
        "X = X / 255.\n",
        "\n",
        "# GET VALIDATION DATASET FROM TRAINING SET\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=13, shuffle=True)\n",
        "\n",
        "\n",
        "# PRINTS TO MAKE SURE\n",
        "print(\"X_train.shape = {}\".format(X_train.shape))\n",
        "print(\"y_train.shape = {}\".format(Y_train.shape))\n",
        "print()\n",
        "print(\"X_val.shape = {}\".format(X_val.shape))\n",
        "print(\"y_val.shape = {}\".format(Y_val.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape = (51000, 28, 28, 1)\n",
            "y_train.shape = (51000, 10)\n",
            "\n",
            "X_val.shape = (9000, 28, 28, 1)\n",
            "y_val.shape = (9000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKp8i0saeSm"
      },
      "source": [
        "#Classifier Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU0xRMnvaiVT"
      },
      "source": [
        "###Create Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXDdo-5fWzxs"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def load_keras_model(model_path):\n",
        "    \"\"\" Function used to load a model from a specific path \"\"\"\n",
        "\n",
        "    # just save the model\n",
        "    return load_model(model_path)\n",
        "\n",
        "def create_classifier(rows, columns, encoder, units):\n",
        "    \"\"\"\n",
        "    Function that given the encoder part and the dense part of a classifier, creates a \"Model\"\n",
        "    (Keras object) that represents a classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    # define the input\n",
        "    input = Input(shape=(rows, columns, 1))\n",
        "    x = input\n",
        "\n",
        "    # pass the input through the encoder\n",
        "    x = encoder(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # pass then the result through the dense layer\n",
        "    x = Dense(units=units, activation='relu')(x)\n",
        "    x = Dense(units=10, activation='softmax')(x)\n",
        "\n",
        "    # create the model and return it\n",
        "    classifier = Model(input, x, name=\"classifier\")\n",
        "    return classifier"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF6lFJ7Sakbx"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "net7GFC1XJoo",
        "outputId": "a3d8f498-888f-48e7-924c-3eb356295deb"
      },
      "source": [
        "encoder1_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Uni\", \"Project\", \"e4_7_64_15_16.h5\")\n",
        "encoder2_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Uni\", \"Project\", \"e3_7_64_15_16_T.h5\")\n",
        "encoder_path = encoder1_path\n",
        "# encoder_path = encoder2_path\n",
        "encoder = load_keras_model(encoder_path)\n",
        "\n",
        "units, epochs, batch_size = configuration\n",
        "\n",
        "encoder.trainable = False\n",
        "\n",
        "classifier = create_classifier(rows, columns, encoder, units)\n",
        "classifier.summary()\n",
        "\n",
        "callback = ReduceLROnPlateau(monitor=\"val_loss\", factor=1.0/2, patience=4, min_delta=0.005,\n",
        "                              cooldown=0, min_lr=1e-8, verbose=1)\n",
        "\n",
        "classifier.compile(optimizer=optimizers.Adam(1e-3), loss=\"categorical_crossentropy\", metrics=[\"categorical_crossentropy\", \"accuracy\"])\n",
        "\n",
        "history = classifier.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
        "                          shuffle=True, validation_data=(X_val, Y_val),\n",
        "                          callbacks=[callback])\n",
        "\n",
        "# fine-tune\n",
        "encoder.trainable = True\n",
        "\n",
        "history = classifier.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
        "                          shuffle=True, validation_data=(X_val, Y_val),\n",
        "                          callbacks=[callback])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 7, 7, 64)          606528    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                200768    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 807,946\n",
            "Trainable params: 201,418\n",
            "Non-trainable params: 606,528\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "3188/3188 [==============================] - 20s 4ms/step - loss: 0.3511 - categorical_crossentropy: 0.3511 - accuracy: 0.8939 - val_loss: 0.2219 - val_categorical_crossentropy: 0.2219 - val_accuracy: 0.9358\n",
            "Epoch 2/25\n",
            "3188/3188 [==============================] - 12s 4ms/step - loss: 0.1393 - categorical_crossentropy: 0.1393 - accuracy: 0.9575 - val_loss: 0.1173 - val_categorical_crossentropy: 0.1173 - val_accuracy: 0.9661\n",
            "Epoch 3/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.1065 - categorical_crossentropy: 0.1065 - accuracy: 0.9683 - val_loss: 0.1444 - val_categorical_crossentropy: 0.1444 - val_accuracy: 0.9603\n",
            "Epoch 4/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0990 - categorical_crossentropy: 0.0990 - accuracy: 0.9699 - val_loss: 0.1125 - val_categorical_crossentropy: 0.1125 - val_accuracy: 0.9667\n",
            "Epoch 5/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0777 - categorical_crossentropy: 0.0777 - accuracy: 0.9763 - val_loss: 0.1117 - val_categorical_crossentropy: 0.1117 - val_accuracy: 0.9700\n",
            "Epoch 6/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0679 - categorical_crossentropy: 0.0679 - accuracy: 0.9789 - val_loss: 0.1229 - val_categorical_crossentropy: 0.1229 - val_accuracy: 0.9667\n",
            "Epoch 7/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0640 - categorical_crossentropy: 0.0640 - accuracy: 0.9802 - val_loss: 0.0960 - val_categorical_crossentropy: 0.0960 - val_accuracy: 0.9751\n",
            "Epoch 8/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0564 - categorical_crossentropy: 0.0564 - accuracy: 0.9825 - val_loss: 0.1193 - val_categorical_crossentropy: 0.1193 - val_accuracy: 0.9701\n",
            "Epoch 9/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0536 - categorical_crossentropy: 0.0536 - accuracy: 0.9831 - val_loss: 0.1313 - val_categorical_crossentropy: 0.1313 - val_accuracy: 0.9663\n",
            "Epoch 10/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0553 - categorical_crossentropy: 0.0553 - accuracy: 0.9826 - val_loss: 0.1226 - val_categorical_crossentropy: 0.1226 - val_accuracy: 0.9699\n",
            "Epoch 11/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0516 - categorical_crossentropy: 0.0516 - accuracy: 0.9839 - val_loss: 0.1796 - val_categorical_crossentropy: 0.1796 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 12/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0284 - categorical_crossentropy: 0.0284 - accuracy: 0.9904 - val_loss: 0.1044 - val_categorical_crossentropy: 0.1044 - val_accuracy: 0.9738\n",
            "Epoch 13/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0211 - categorical_crossentropy: 0.0211 - accuracy: 0.9930 - val_loss: 0.1099 - val_categorical_crossentropy: 0.1099 - val_accuracy: 0.9772\n",
            "Epoch 14/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0192 - categorical_crossentropy: 0.0192 - accuracy: 0.9936 - val_loss: 0.0927 - val_categorical_crossentropy: 0.0927 - val_accuracy: 0.9779\n",
            "Epoch 15/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0161 - categorical_crossentropy: 0.0161 - accuracy: 0.9942 - val_loss: 0.0998 - val_categorical_crossentropy: 0.0998 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 16/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0083 - categorical_crossentropy: 0.0083 - accuracy: 0.9976 - val_loss: 0.0900 - val_categorical_crossentropy: 0.0900 - val_accuracy: 0.9807\n",
            "Epoch 17/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0069 - categorical_crossentropy: 0.0069 - accuracy: 0.9983 - val_loss: 0.0959 - val_categorical_crossentropy: 0.0959 - val_accuracy: 0.9787\n",
            "Epoch 18/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0077 - categorical_crossentropy: 0.0077 - accuracy: 0.9985 - val_loss: 0.0970 - val_categorical_crossentropy: 0.0970 - val_accuracy: 0.9794\n",
            "Epoch 19/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0053 - categorical_crossentropy: 0.0053 - accuracy: 0.9988 - val_loss: 0.0925 - val_categorical_crossentropy: 0.0925 - val_accuracy: 0.9789\n",
            "Epoch 20/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0041 - categorical_crossentropy: 0.0041 - accuracy: 0.9989 - val_loss: 0.0958 - val_categorical_crossentropy: 0.0958 - val_accuracy: 0.9790\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 21/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0035 - categorical_crossentropy: 0.0035 - accuracy: 0.9991 - val_loss: 0.0946 - val_categorical_crossentropy: 0.0946 - val_accuracy: 0.9800\n",
            "Epoch 22/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0022 - categorical_crossentropy: 0.0022 - accuracy: 0.9997 - val_loss: 0.0976 - val_categorical_crossentropy: 0.0976 - val_accuracy: 0.9811\n",
            "Epoch 23/25\n",
            "3188/3188 [==============================] - 12s 4ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018 - accuracy: 0.9998 - val_loss: 0.0997 - val_categorical_crossentropy: 0.0997 - val_accuracy: 0.9812\n",
            "Epoch 24/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0018 - categorical_crossentropy: 0.0018 - accuracy: 0.9998 - val_loss: 0.0995 - val_categorical_crossentropy: 0.0995 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 25/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0014 - categorical_crossentropy: 0.0014 - accuracy: 0.9999 - val_loss: 0.0988 - val_categorical_crossentropy: 0.0988 - val_accuracy: 0.9810\n",
            "Epoch 1/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0012 - categorical_crossentropy: 0.0012 - accuracy: 0.9999 - val_loss: 0.0990 - val_categorical_crossentropy: 0.0990 - val_accuracy: 0.9806\n",
            "Epoch 2/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0011 - categorical_crossentropy: 0.0011 - accuracy: 0.9999 - val_loss: 0.1010 - val_categorical_crossentropy: 0.1010 - val_accuracy: 0.9807\n",
            "Epoch 3/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 0.0010 - categorical_crossentropy: 0.0010 - accuracy: 0.9999 - val_loss: 0.1032 - val_categorical_crossentropy: 0.1032 - val_accuracy: 0.9804\n",
            "Epoch 4/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 8.6505e-04 - categorical_crossentropy: 8.6505e-04 - accuracy: 0.9999 - val_loss: 0.1045 - val_categorical_crossentropy: 0.1045 - val_accuracy: 0.9800\n",
            "Epoch 5/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 8.1191e-04 - categorical_crossentropy: 8.1191e-04 - accuracy: 0.9999 - val_loss: 0.1037 - val_categorical_crossentropy: 0.1037 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 6/25\n",
            "3188/3188 [==============================] - 12s 4ms/step - loss: 6.3881e-04 - categorical_crossentropy: 6.3881e-04 - accuracy: 0.9999 - val_loss: 0.1035 - val_categorical_crossentropy: 0.1035 - val_accuracy: 0.9810\n",
            "Epoch 7/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 5.7058e-04 - categorical_crossentropy: 5.7058e-04 - accuracy: 1.0000 - val_loss: 0.1044 - val_categorical_crossentropy: 0.1044 - val_accuracy: 0.9809\n",
            "Epoch 8/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 5.5124e-04 - categorical_crossentropy: 5.5124e-04 - accuracy: 1.0000 - val_loss: 0.1056 - val_categorical_crossentropy: 0.1056 - val_accuracy: 0.9810\n",
            "Epoch 9/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 5.0502e-04 - categorical_crossentropy: 5.0502e-04 - accuracy: 1.0000 - val_loss: 0.1066 - val_categorical_crossentropy: 0.1066 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 10/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 4.4628e-04 - categorical_crossentropy: 4.4628e-04 - accuracy: 1.0000 - val_loss: 0.1058 - val_categorical_crossentropy: 0.1058 - val_accuracy: 0.9811\n",
            "Epoch 11/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 4.2608e-04 - categorical_crossentropy: 4.2608e-04 - accuracy: 1.0000 - val_loss: 0.1060 - val_categorical_crossentropy: 0.1060 - val_accuracy: 0.9812\n",
            "Epoch 12/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 4.1491e-04 - categorical_crossentropy: 4.1491e-04 - accuracy: 1.0000 - val_loss: 0.1059 - val_categorical_crossentropy: 0.1059 - val_accuracy: 0.9811\n",
            "Epoch 13/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.9925e-04 - categorical_crossentropy: 3.9925e-04 - accuracy: 1.0000 - val_loss: 0.1069 - val_categorical_crossentropy: 0.1069 - val_accuracy: 0.9810\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 14/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.6712e-04 - categorical_crossentropy: 3.6712e-04 - accuracy: 1.0000 - val_loss: 0.1071 - val_categorical_crossentropy: 0.1071 - val_accuracy: 0.9810\n",
            "Epoch 15/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.5989e-04 - categorical_crossentropy: 3.5989e-04 - accuracy: 1.0000 - val_loss: 0.1073 - val_categorical_crossentropy: 0.1073 - val_accuracy: 0.9810\n",
            "Epoch 16/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.5639e-04 - categorical_crossentropy: 3.5639e-04 - accuracy: 1.0000 - val_loss: 0.1075 - val_categorical_crossentropy: 0.1075 - val_accuracy: 0.9807\n",
            "Epoch 17/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.4890e-04 - categorical_crossentropy: 3.4890e-04 - accuracy: 1.0000 - val_loss: 0.1076 - val_categorical_crossentropy: 0.1076 - val_accuracy: 0.9806\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "Epoch 18/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.3519e-04 - categorical_crossentropy: 3.3519e-04 - accuracy: 1.0000 - val_loss: 0.1079 - val_categorical_crossentropy: 0.1079 - val_accuracy: 0.9811\n",
            "Epoch 19/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.3264e-04 - categorical_crossentropy: 3.3264e-04 - accuracy: 1.0000 - val_loss: 0.1079 - val_categorical_crossentropy: 0.1079 - val_accuracy: 0.9811\n",
            "Epoch 20/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.2787e-04 - categorical_crossentropy: 3.2787e-04 - accuracy: 1.0000 - val_loss: 0.1081 - val_categorical_crossentropy: 0.1081 - val_accuracy: 0.9810\n",
            "Epoch 21/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.2692e-04 - categorical_crossentropy: 3.2692e-04 - accuracy: 1.0000 - val_loss: 0.1083 - val_categorical_crossentropy: 0.1083 - val_accuracy: 0.9811\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "Epoch 22/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.1938e-04 - categorical_crossentropy: 3.1938e-04 - accuracy: 1.0000 - val_loss: 0.1083 - val_categorical_crossentropy: 0.1083 - val_accuracy: 0.9811\n",
            "Epoch 23/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.1736e-04 - categorical_crossentropy: 3.1736e-04 - accuracy: 1.0000 - val_loss: 0.1083 - val_categorical_crossentropy: 0.1083 - val_accuracy: 0.9811\n",
            "Epoch 24/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.1614e-04 - categorical_crossentropy: 3.1614e-04 - accuracy: 1.0000 - val_loss: 0.1083 - val_categorical_crossentropy: 0.1083 - val_accuracy: 0.9811\n",
            "Epoch 25/25\n",
            "3188/3188 [==============================] - 13s 4ms/step - loss: 3.1504e-04 - categorical_crossentropy: 3.1504e-04 - accuracy: 1.0000 - val_loss: 0.1084 - val_categorical_crossentropy: 0.1084 - val_accuracy: 0.9811\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT283vZe-NpT"
      },
      "source": [
        "#Write the prediction for each image in the dataset to a txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGm_Sd9z-ZVM"
      },
      "source": [
        "def separate_to_clusters(Y):\n",
        "    clusters = [[] for i in range(10)]\n",
        "\n",
        "    for i in range(len(Y)):\n",
        "        clusters[Y[i]].append(i)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def produce_label_file(clusters, file_path):\n",
        "    file = open(file_path, 'w+')\n",
        "\n",
        "    for i in range(len(clusters)):\n",
        "        file.write(\"CLUSTER-{} {{ size: {}\".format(i+1, len(clusters[i])))\n",
        "\n",
        "        for index in clusters[i]:\n",
        "            file.write(\", {}\".format(index))\n",
        "\n",
        "        file.write(\"}\\n\")\n",
        "\n",
        "    file.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDBHzp9f-q5i"
      },
      "source": [
        "Y_prob = classifier.predict(X)\n",
        "Y_pred = np.round(Y_prob)\n",
        "Y_unbin = np.argmax(Y_pred, 1)\n",
        "\n",
        "clusters = separate_to_clusters(Y_unbin)\n",
        "output_path = os.path.join(\".\", \"drive\", \"My Drive\", \"Uni\", \"Project\", \"cs64_25_16.txt\")\n",
        "produce_label_file(clusters, output_path)"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}